# -*- coding: utf-8 -*-
"""RAG-Powered Q&A System for Duolingo English Test Documentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eG9llk_orSbuWz5XfNyeo4zE8ELdmxsj

**RAG-Powered Q&A System for Duolingo English Test Documentation**

This notebook implements a Retrieval-Augmented Generation (RAG) system designed to answer domain-specific questions using official Duolingo English Test (DET) documentation. The system ingests PDF documents, processes and chunks the text, generates semantic embeddings, and stores them in a FAISS vector database to enable efficient similarity-based retrieval.

When a user submits a question, the system retrieves the most relevant document chunks and generates a grounded response based solely on the retrieved context. This approach improves factual accuracy and reduces hallucination compared to standalone language models.

The project demonstrates the following components:



*   Document loading and preprocessing
*   Text chunking for semantic search


*  Embedding generation using sentence-transformers
*   Vector storage and similarity search with FAISS
*   Context-grounded question answering


This implementation showcases how Retrieval-Augmented Generation can be applied to structured exam documentation to build an intelligent, domain-aware assistant.
"""

!pip install requests==2.32.4 --quiet

!pip install -q \
langchain \
langchain-community \
faiss-cpu \
sentence-transformers \
pypdf \
requests==2.32.4

from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from google.colab import files
uploaded = files.upload()

DATA_DIR = Path("/content")
def load_documents(data_dir:Path):
  docs = []
  for fp in data_dir.glob("*"):
    if fp.suffix.lower() == ".pdf":
      docs.extend(PyPDFLoader(str(fp)).load())
    elif fp.suffix.lower() in [".txt", ".md"]:
      docs.extend(TextLoader(str(fp),encoding="utf-8").load())
  return docs

raw_docs = load_documents(DATA_DIR)
print("Loaded docs:", len(raw_docs))

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150
)
chunks = splitter.split_documents(raw_docs)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k":4})

print("Chunks created:",len(chunks))

q = "What is the test structure and how long does the Duolingo English Test take?"
hits = retriever.get_relevant_documents(q)

for i,d in enumerate(hits,1):
  src = Path(d.metadata.get("source","")).name
  page = d.metadata.get("page",None)
  print(f"\n--- Result {i} (source={src},page={page})---\n")
  print(d.page_content[:600])

import json

def clean_notebook(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        notebook = json.load(f)
    if "metadata" in notebook and "widgets" in notebook["metadata"]:
        del notebook["metadata"]["widgets"]
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(notebook, f, indent=2)

# Usage
# clean_notebook("your_notebook_name.ipynb")